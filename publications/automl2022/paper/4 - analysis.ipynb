{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649466a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGOS = [\"rf\", \"lda\", \"pca\"]\n",
    "ALGO_NAMES = [\"RF\", \"LDA RF\", \"PCA RF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a5b43",
   "metadata": {},
   "source": [
    "# Total CPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.read_csv(\"comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da50e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_hours = sum(df_comparison[\"traintime_total_max\"]) / 3600\n",
    "cpu_days = cpu_hours / 24\n",
    "print(cpu_hours, \"hours\")\n",
    "print(cpu_days, \"days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,3),  gridspec_kw={'width_ratios': [1.2, 1, 1]})\n",
    "ct = pd.crosstab(df_comparison[\"best_choice_test\"], df_comparison.rename(columns={\"choice\": \"SORF Choice\"})[\"SORF Choice\"])\n",
    "\n",
    "permutation = [1, 2, 0]\n",
    "\n",
    "# plot comparison of RF against LDA RF and PCA RF\n",
    "df_rfcomparison = df_comparison.groupby(\"openmlid\")[\"rf_test\", \"lda_test\", \"pca_test\"].mean()\n",
    "ax[0].scatter(df_rfcomparison[\"rf_test\"], df_rfcomparison[\"lda_test\"], color=\"C0\", s=5)\n",
    "ax[0].scatter(df_rfcomparison[\"rf_test\"], df_rfcomparison[\"pca_test\"], color=\"C1\", s=5)\n",
    "ax[0].plot([0.2,1], [0.2,1], linewidth=1, linestyle=\"--\", color=\"black\")\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"Accuracy of standard RF\")\n",
    "ax[0].set_ylabel(\"Accuracy of\\nLDA RF (blue) and PCA RF(orange)\")\n",
    "\n",
    "ct = ct.rename(columns={a: n for a, n in zip(ALGOS, ALGO_NAMES)})\n",
    "ct.plot(kind='bar', stacked=True, rot=0, ax=ax[1])\n",
    "ct_normalized = ct.values / np.sum(ct.values)\n",
    "sns.heatmap(ct_normalized, annot=True, ax = ax[2], vmax=0.33, cmap=\"Greens\")\n",
    "for a in ax[1:]:\n",
    "    a.set_xlabel(\"Best Choice\")\n",
    "ax[1].set_xticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[1].set_title(\"SORF and Best Choices\\n(in absolute numbers)\")\n",
    "\n",
    "ax[2].set_ylabel(\"SORF Choice\")\n",
    "ax[2].set_xticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[2].set_yticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[2].set_title(\"Confusion Matrix of SORF\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/confusion.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0255dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(10, 2.5),  gridspec_kw={'width_ratios': [1.5, 1.5, 1, 1]})\n",
    "\n",
    "Z = []\n",
    "for i, a_choice in enumerate(ALGOS):\n",
    "    df_selection = df_comparison[(df_comparison[\"choice\"] == a_choice)]\n",
    "    Z.append([np.mean(df_selection[\"traintime_\" + a_tree + \"_act\"] / df_selection[\"traintime_\" + a_tree + \"_max\"]) for a_tree in ALGOS])\n",
    "Z = np.array(Z)\n",
    "Z = np.column_stack([Z, np.sum(Z, axis=1) / 3])\n",
    "\n",
    "ax = axes[0]\n",
    "sns.heatmap(Z, annot=True, ax = ax, cmap=\"Reds\")\n",
    "ax.set_xticklabels(ALGO_NAMES + [\"Total\"])\n",
    "ax.set_yticklabels(ALGO_NAMES)\n",
    "ax.set_ylabel(\"Chosen Forest Type\")\n",
    "ax.set_xlabel(\"Training time spent per tree type\\n(relative to maximum possible)\")\n",
    "\n",
    "Z = []\n",
    "for i, a_choice in enumerate(ALGOS):\n",
    "    df_selection = df_comparison[(df_comparison[\"choice\"] == a_choice)]\n",
    "    Z.append([int(np.round(np.mean(df_selection[\"trees_\" + a_tree]))) for a_tree in ALGOS])\n",
    "Z = np.array(Z,dtype=int)\n",
    "Z = np.column_stack([Z, np.sum(Z, axis=1)])\n",
    "ax = axes[1]\n",
    "sns.heatmap(Z, annot=True, ax = ax, cmap=\"Reds\", fmt='g')\n",
    "ax.set_xticklabels(ALGO_NAMES + [\"Total\"])\n",
    "ax.set_yticklabels(ALGO_NAMES)\n",
    "ax.set_ylabel(\"Chosen forest type\")\n",
    "ax.set_xlabel(\"Numbers of trees grown per tree type\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compressions_in_time = [[np.mean(g[\"time_compression\"]) for i, g in df_comparison[df_comparison[\"choice\"] == a].groupby(\"openmlid\")] for a in ALGOS] + [[np.mean(g[\"time_compression\"]) for i, g in df_comparison.groupby(\"openmlid\")]]\n",
    "compressions_in_num_trees = [[np.mean(g[\"tree_compression\"]) for i, g in df_comparison[df_comparison[\"choice\"] == a].groupby(\"openmlid\")] for a in ALGOS] + [[np.mean(g[\"tree_compression\"]) for i, g in df_comparison.groupby(\"openmlid\")]]\n",
    "\n",
    "for a, values in zip(axes[2:], [compressions_in_time, compressions_in_num_trees]):\n",
    "    a.boxplot(values)\n",
    "    a.set_ylim([0, 1])\n",
    "    a.set_xticklabels(ALGO_NAMES + [\"Total\"], rotation=90)\n",
    "    a.axhline(0.5, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    a.axhline(0.25, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    a.axvline(4.5, color=\"black\", linewidth=1)\n",
    "axes[2].set_ylabel(\"Time Compression\")\n",
    "axes[3].set_ylabel(\"Tree Compression\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/computations.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe5b48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_performance_figure_and_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances_with_sorf = {a : [] for a in algo_names}\n",
    "    significances_with_best = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        \n",
    "        # extract the scores of the algorithms on this dataset\n",
    "        perf_selfopt = df_dataset[\"selfopt_test\"]\n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[comp + \"_test\"]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances_with_sorf[comp].append(significant)\n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "\n",
    "        # compute mean scores of the algorithms on dataset\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = max(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        \n",
    "        \n",
    "        # determine significant differences to best\n",
    "        best_algo = (algo_names + [\"selfopt\"])[np.argmax(scores_on_dataset)]\n",
    "        perf_best = df_dataset[best_algo + \"_test\"]\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[comp + \"_test\"]\n",
    "            if np.linalg.norm(perf_comp - perf_best) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_best).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances_with_best[comp].append(significant)\n",
    "        \n",
    "        \n",
    "        # format entries\n",
    "        formatted_vals = [f\"{np.round(100 * np.mean(v), 2)}$\\pm${np.round(100 * np.std(v), 1)}\" for i, v in enumerate(data_base)]\n",
    "        #imps.append(scores_on_dataset[1] - scores_on_dataset[0])\n",
    "        for i, val in enumerate(formatted_vals):\n",
    "            if i in best_indices:\n",
    "                formatted_vals[i] = \"\\\\textbf{\" + val + \"}\"\n",
    "            elif not significant:\n",
    "                formatted_vals[i] = \"\\\\underline{\" + val + \"}\"\n",
    "\n",
    "        rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    imps = np.array(imps)\n",
    "    \n",
    "    for a in algo_names:\n",
    "        significances_with_sorf[a] = np.array(significances_with_sorf[a])\n",
    "        significances_with_best[a] = np.array(significances_with_best[a])\n",
    "    \n",
    "    \n",
    "    colors = {\n",
    "        \"rf\": \"C2\",\n",
    "        \"lda\": \"C0\",\n",
    "        \"pca\": \"C1\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3),  gridspec_kw={'width_ratios': [1, 1]})\n",
    "    \n",
    "    \n",
    "    for i, a in enumerate([\"rf\", \"lda\", \"pca\"]):\n",
    "        ax1.scatter(scores[significances_with_sorf[a],i], scores[significances_with_sorf[a],-1], s=20, color=colors[a])\n",
    "        ax1.scatter(scores[~significances_with_sorf[a],i], scores[~significances_with_sorf[a],-1], s=20, facecolors=\"None\", color=colors[a])\n",
    "    \n",
    "    ax1.plot([0.2,1], [0.2,1], linestyle=\"dotted\", color=\"black\")\n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel(\"Accuracy of Standard RF (blue), LDA RF (orange), and PCA RF (green).\\nBullets show significant and circle statistically not significant differences.\")\n",
    "    ax1.set_ylabel(\"Accuracy of Self-Optimized RF\")\n",
    "    \n",
    "    imp_concated = []\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[:,i])\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[significances_with_sorf[algo],i])\n",
    "    \n",
    "    imp_concated.append(np.min(imps, axis=1))\n",
    "    \n",
    "    ax2.boxplot(imp_concated, vert=False)\n",
    "    ax2.axvline(-0.01, linestyle=\"dotted\", color=\"red\", linewidth=1)\n",
    "    ax2.axvline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.01, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.03, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.set_xlabel(\"Accuracy improvement achieved by SORF compared to ...\")\n",
    "    ax2.set_yticklabels(ALGO_NAMES + [name + \" (sign.)\" for name in ALGO_NAMES] + [\"Oracle\"])\n",
    "    #ax2.scatter(list(range(len(imps))), imps)\n",
    "    #ax2.hist(imps, bins=200)\n",
    "    #ax2.set_yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(np.mean(imp_concated[-1]))\n",
    "    \n",
    "    return (fig, ax), pd.DataFrame(rows, columns=[\"openmlid\"] + ALGO_NAMES + [\"SORF\"])\n",
    "    \n",
    "(fig, ax), df_results = get_performance_figure_and_table_for_paper(df_comparison)\n",
    "fig.savefig(\"plots/performance.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ce8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_results.head(48).to_latex(index = False, escape = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd7684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_results.tail(48).to_latex(index = False, escape = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada58fd",
   "metadata": {},
   "source": [
    "# Datasets With Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6bc625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "datasets = [int(i) for i in sorted(pd.unique(df_results[\"openmlid\"]))]\n",
    "rows = []\n",
    "for openmlid in tqdm(datasets):\n",
    "    \n",
    "    # load dataset object\n",
    "    dataset = openml.datasets.get_dataset(openmlid)\n",
    "    \n",
    "    # analyze columns of data\n",
    "    if False:\n",
    "        dfDataset = dataset.get_data()[0]\n",
    "        types = dfDataset.dtypes\n",
    "        cnt_normalized = 0\n",
    "        cnt_zero_mean = 0\n",
    "        cnt_one_std = 0\n",
    "        cnt_numerical = 0\n",
    "        for i, col in enumerate(dfDataset.columns):\n",
    "            if \"float\" in str(types[i]):\n",
    "                cnt_numerical += 1\n",
    "                vals = dfDataset[col].values\n",
    "                is_normalized = np.round(min(vals), 3) == 0 and np.round(max(vals), 3) == 1\n",
    "                is_zero_mean = np.round(np.mean(vals), 3) == 0\n",
    "                is_one_std = np.round(np.std(vals), 3) == 1\n",
    "                if is_normalized:\n",
    "                    cnt_normalized += 1\n",
    "                if is_zero_mean:\n",
    "                    cnt_zero_mean += 1\n",
    "                if is_one_std:\n",
    "                    cnt_one_std += 1\n",
    "        if cnt_numerical > 0:\n",
    "            feature_stats_entries = [str(v) + \"\\%\" for v in np.round(100 * np.array([cnt_normalized / cnt_numerical, cnt_zero_mean / cnt_numerical, cnt_one_std / cnt_numerical]), 0).astype(int)]\n",
    "        else:\n",
    "            feature_stats_entries = 3 * [\"n/a\"]\n",
    "    \n",
    "    \n",
    "    num_instances = int(dataset.qualities[\"NumberOfInstances\"])\n",
    "    num_features = len(dataset.features) - 1\n",
    "    num_features_numeric = int(dataset.qualities[\"NumberOfNumericFeatures\"])\n",
    "    num_classes = int(dataset.qualities[\"NumberOfClasses\"])\n",
    "    #min_class_percentage = str(int(dataset.qualities[\"MinorityClassPercentage\"])) + \"\\%\"\n",
    "    #maj_class_percentage = str(int(dataset.qualities[\"MajorityClassPercentage\"])) + \"\\%\"\n",
    "    missing_values = str(int(dataset.qualities[\"PercentageOfMissingValues\"])) + \"\\%\"\n",
    "    \n",
    "    row = [openmlid, dataset.name[:20].replace(\"_\", \"\\\\_\"), num_instances, num_features, num_features_numeric, num_classes, missing_values]\n",
    "    rows.append(row)\n",
    "dfDatasets = pd.DataFrame(rows, columns=[\"openmlid\", \"name\", \"instances\", \"features\", \"numeric features\", \"classes\", \"\\% missing\"]).sort_values(\"openmlid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0acc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfDatasets[[\"openmlid\", \"name\", \"instances\", \"features\", \"classes\"]].head(48).to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfDatasets[[\"openmlid\", \"name\", \"instances\", \"features\", \"classes\"]].tail(48).to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a67382",
   "metadata": {},
   "source": [
    "# Tables for Runtimes and Trained Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff133bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feeb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runtime_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances_with_sorf = {a : [] for a in algo_names}\n",
    "    significances_with_best = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        \n",
    "        # extract the scores of the algorithms on this dataset\n",
    "        perf_selfopt = df_dataset[\"traintime_total_act\"]\n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[\"traintime_\" + comp + \"_max\"]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances_with_sorf[comp].append(significant)\n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "\n",
    "        # compute mean scores of the algorithms on dataset\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = min(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        \n",
    "        \n",
    "        # format entries\n",
    "        formatted_vals = [f\"{int(np.round(np.mean(v)))}$\\pm${int(np.round(np.std(v)))}\" for i, v in enumerate(data_base)]\n",
    "        rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=[\"openmlid\"] + ALGO_NAMES + [\"SORF\"])\n",
    "    \n",
    "df_results = get_runtime_table_for_paper(df_comparison)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b93cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_results.head(48).to_latex(index = False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.tail(48).to_latex(index = False, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances_with_sorf = {a : [] for a in algo_names}\n",
    "    significances_with_best = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        \n",
    "        # extract the scores of the algorithms on this dataset\n",
    "        perf_selfopt = df_dataset[\"trees_rf\"] + df_dataset[\"trees_lda\"] + df_dataset[\"trees_pca\"]\n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[\"trees_\" + comp]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances_with_sorf[comp].append(significant)\n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "\n",
    "        # compute mean scores of the algorithms on dataset\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = min(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        \n",
    "        \n",
    "        # format entries\n",
    "        formatted_vals = [f\"{int(np.round(np.mean(v)))}$\\pm${int(np.round(np.std(v)))}\" for i, v in enumerate(data_base)]\n",
    "        rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=[\"openmlid\", \"Standard Trees\", \"LDA Trees\", \"PCA Trees\", \"Total\"])\n",
    "    \n",
    "df_results = get_tree_table_for_paper(df_comparison)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7dd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.head(48).to_latex(index=False,escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad53e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.tail(48).to_latex(index=False,escape=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
