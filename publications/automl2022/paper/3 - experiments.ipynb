{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649466a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "from selfoptforest import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGOS = [\"rf\", \"lda\", \"pca\"]\n",
    "ALGO_NAMES = [\"RF\", \"LDA RF\", \"PCA RF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94306a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark = pd.read_csv(\"benchmark.csv\")\n",
    "df_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark.query(\"openmlid == 60 and size == 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1f10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bm = Benchmark(df_benchmark)\n",
    "\n",
    "\n",
    "seeds = list(range(20))\n",
    "datasets = sorted(pd.unique(df_benchmark[\"openmlid\"]))\n",
    "\n",
    "rows = []\n",
    "\n",
    "MAX_TREES = 100\n",
    "\n",
    "for i, openmlid in enumerate(tqdm(datasets)):\n",
    "    for j, seed in enumerate(seeds):\n",
    "\n",
    "        print(openmlid, \" (seed \" + str(seed) + \")\\n-------------------------------\")\n",
    "        \n",
    "        if np.count_nonzero((df_benchmark[\"openmlid\"] == openmlid) & (df_benchmark[\"seed\"] == seed)) == 0:\n",
    "            print(\"Skipping, no data for this seed.\")\n",
    "            continue\n",
    "        \n",
    "        bm.reset(openmlid, seed)\n",
    "        rf = SelfOptRF()\n",
    "        rf.simulate_training_with_benchmark(bm, max_forest_size = MAX_TREES)\n",
    "        \n",
    "        try:\n",
    "            choice = rf.choice\n",
    "            print(f\"CHOICE: {choice}\")\n",
    "            print(\"TRAIN TIME SELF OPT:\", bm.get_current_train_time())\n",
    "            print(\"TRAIN TIMES PURE RFS: \", [bm.get_train_time(name, MAX_TREES) for name in bm.algos])\n",
    "            print(\"WORST CASE TIME: \", sum([bm.get_train_time(name, MAX_TREES) for name in bm.algos]))\n",
    "            try:\n",
    "                test_performance = np.round(np.mean([bm.get_test_performance_at_size(choice, MAX_TREES + i) for i in range(-2, 3)]), 4)\n",
    "            except:\n",
    "                print(\"error in retrieving performance, going to next dataset!\")\n",
    "                continue\n",
    "            print(f\"Test performance of choice: {test_performance}\")\n",
    "\n",
    "            best_performance_test = 0\n",
    "            best_performance_oob = 0\n",
    "            best_choice_test = None\n",
    "            best_choice_oob = None\n",
    "            rf_performance_test = 0\n",
    "            rf_performance_oob = 0\n",
    "\n",
    "            row = [openmlid, seed]\n",
    "            for algo in bm.algos:\n",
    "                performance_test = np.round(np.mean([bm.get_test_performance_at_size(algo, MAX_TREES + i) for i in range(-2,3)]), 4)\n",
    "                performance_oob = np.round(np.mean([bm.get_oob_performance_at_size(algo, MAX_TREES + i) for i in range(-2,3)]), 4)\n",
    "                row.append(performance_oob)\n",
    "                row.append(performance_test)\n",
    "                if algo == \"rf\":\n",
    "                    rf_performance_test = performance_test\n",
    "                    rf_performance_oob = performance_oob\n",
    "                if performance_test > best_performance_test:\n",
    "                    best_performance_test = performance_test\n",
    "                    best_choice_test = algo\n",
    "                if performance_oob > best_performance_oob:\n",
    "                    best_performance_oob = performance_oob\n",
    "                    best_choice_oob = algo\n",
    "\n",
    "                print(algo + \"\\t\" + str(performance_oob), performance_test, np.round(performance_test-  performance_oob, 3))\n",
    "\n",
    "            gap = best_performance_test - test_performance\n",
    "            improvement_over_rf = np.round(test_performance - rf_performance_test, 3)\n",
    "            train_times = [bm.get_current_train_time(a) for a in bm.algos]\n",
    "            trained_trees = [bm.indices[i] for i, a in enumerate(bm.algos)]\n",
    "            max_train_times = [bm.get_train_time(a, MAX_TREES) for a in bm.algos]\n",
    "            time_compression = np.round(sum(train_times) / sum(max_train_times), 2)\n",
    "            \n",
    "            print(f\"Number of trained trees: {bm.indices}\")\n",
    "            print(f\"Actual  Train Times: {train_times}\")\n",
    "            print(f\"Maximum Train Times: {max_train_times}\")\n",
    "            print(\"Time compression:\", np.round(time_compression, 2))\n",
    "            \n",
    "            # sanity check\n",
    "            for k in range(3):\n",
    "                if train_times[k] > max_train_times[k]:\n",
    "                    raise Exception()\n",
    "\n",
    "            rows.append(row + [choice, best_choice_oob, best_choice_test, gap, improvement_over_rf] + train_times + max_train_times + trained_trees)\n",
    "            print(\"best choice found:\", choice == best_choice_oob, choice == best_choice_test, f\"Test gap to best: {np.round(gap, 4)}. Test improvement over RF: {improvement_over_rf}\")\n",
    "        except:\n",
    "            \n",
    "            print(\"!!!!!!!!1 DA IST WAS SCHIEFGEGANGEN!!!!!!!!!!\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc5bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comparison = pd.DataFrame(rows, columns=[\"openmlid\", \"seed\", \"rf_oob\", \"rf_test\", \"lda_oob\", \"lda_test\", \"pca_oob\", \"pca_test\", \"choice\", \"best_choice_oob\", \"best_choice_test\", \"gap\", \"imp\", \"traintime_rf_act\", \"traintime_lda_act\", \"traintime_pca_act\", \"traintime_rf_max\", \"traintime_lda_max\", \"traintime_pca_max\", \"trees_rf\", \"trees_lda\", \"trees_pca\"])\n",
    "gaps_oob = []\n",
    "gaps_test = []\n",
    "imps_oob = []\n",
    "imps_test = []\n",
    "performance_selfopt_oob = []\n",
    "performance_selfopt_test = []\n",
    "for i, row in df_comparison.iterrows():\n",
    "    gap_oob = row[row[\"best_choice_oob\"] + \"_oob\"] - row[row[\"choice\"] + \"_oob\"]\n",
    "    gap_test = row[row[\"best_choice_test\"] + \"_test\"] - row[row[\"choice\"] + \"_test\"]\n",
    "    imp_oob = row[row[\"choice\"] + \"_oob\"] - row[\"rf_oob\"]\n",
    "    imp_test = row[row[\"choice\"] + \"_test\"] - row[\"rf_test\"]\n",
    "    gaps_oob.append(gap_oob)\n",
    "    gaps_test.append(gap_test)\n",
    "    imps_oob.append(imp_oob)\n",
    "    imps_test.append(imp_test)\n",
    "    performance_selfopt_oob.append(row[row[\"choice\"] + \"_oob\"])\n",
    "    performance_selfopt_test.append(row[row[\"choice\"] + \"_test\"])\n",
    "df_comparison[\"selfopt_oob\"] = performance_selfopt_oob\n",
    "df_comparison[\"selfopt_test\"] = performance_selfopt_test\n",
    "df_comparison[\"gap_oob\"] = gaps_oob\n",
    "df_comparison[\"gap_test\"] = gaps_test\n",
    "df_comparison[\"imp_oob\"] = imps_oob\n",
    "df_comparison[\"imp_test\"] = imps_test\n",
    "df_comparison[\"traintime_total_act\"] = df_comparison[\"traintime_rf_act\"] + df_comparison[\"traintime_lda_act\"] + df_comparison[\"traintime_pca_act\"]\n",
    "df_comparison[\"traintime_total_max\"] = df_comparison[\"traintime_rf_max\"] + df_comparison[\"traintime_lda_max\"] + df_comparison[\"traintime_pca_max\"]\n",
    "df_comparison[\"time_compression\"] = df_comparison[\"traintime_total_act\"] / df_comparison[\"traintime_total_max\"]\n",
    "df_comparison[\"tree_compression\"] = (df_comparison[\"trees_rf\"] + df_comparison[\"trees_lda\"] + df_comparison[\"trees_pca\"]) / 300\n",
    "df_comparison.to_csv(\"comparison.csv\", index=False)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a5b43",
   "metadata": {},
   "source": [
    "# Total CPU Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da50e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_hours = sum(df_comparison[\"traintime_total_max\"]) / 3600\n",
    "cpu_days = cpu_hours / 24\n",
    "print(cpu_hours, \"hours\")\n",
    "print(cpu_days, \"days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df_comparison[\"openmlid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02097d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfcomparison = df_comparison.groupby(\"openmlid\")[\"rf_test\", \"lda_test\", \"pca_test\"].mean()\n",
    "df_rfcomparison[np.abs(df_rfcomparison[\"rf_test\"] - df_rfcomparison[\"pca_test\"]) > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,3),  gridspec_kw={'width_ratios': [1.2, 1, 1]})\n",
    "ct = pd.crosstab(df_comparison[\"best_choice_test\"], df_comparison.rename(columns={\"choice\": \"SORF Choice\"})[\"SORF Choice\"])\n",
    "\n",
    "permutation = [1, 2, 0]\n",
    "\n",
    "# plot comparison of RF against LDA RF and PCA RF\n",
    "df_rfcomparison = df_comparison.groupby(\"openmlid\")[\"rf_test\", \"lda_test\", \"pca_test\"].mean()\n",
    "ax[0].scatter(df_rfcomparison[\"rf_test\"], df_rfcomparison[\"lda_test\"], color=\"C0\", s=5)\n",
    "ax[0].scatter(df_rfcomparison[\"rf_test\"], df_rfcomparison[\"pca_test\"], color=\"C1\", s=5)\n",
    "ax[0].plot([0.2,1], [0.2,1], linewidth=1, linestyle=\"--\", color=\"black\")\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"Accuracy of standard RF\")\n",
    "ax[0].set_ylabel(\"Accuracy of\\nLDA RF (blue) and PCA RF(orange)\")\n",
    "\n",
    "ct = ct.rename(columns={a: n for a, n in zip(ALGOS, ALGO_NAMES)})\n",
    "ct.plot(kind='bar', stacked=True, rot=0, ax=ax[1])\n",
    "ct_normalized = ct.values / np.sum(ct.values)\n",
    "sns.heatmap(ct_normalized, annot=True, ax = ax[2], vmax=0.33, cmap=\"Greens\")\n",
    "for a in ax[1:]:\n",
    "    a.set_xlabel(\"Best Choice\")\n",
    "ax[1].set_xticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[1].set_title(\"SORF and Best Choices\\n(in absolute numbers)\")\n",
    "\n",
    "ax[2].set_ylabel(\"SORF Choice\")\n",
    "ax[2].set_xticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[2].set_yticklabels([ALGO_NAMES[i] for i in permutation])\n",
    "ax[2].set_title(\"Confusion Matrix of SORF\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/confusion.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0255dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(10, 2.5),  gridspec_kw={'width_ratios': [1.5, 1.5, 1, 1]})\n",
    "\n",
    "Z = []\n",
    "for i, a_choice in enumerate(ALGOS):\n",
    "    df_selection = df_comparison[(df_comparison[\"choice\"] == a_choice)]\n",
    "    Z.append([np.mean(df_selection[\"traintime_\" + a_tree + \"_act\"] / df_selection[\"traintime_\" + a_tree + \"_max\"]) for a_tree in ALGOS])\n",
    "Z = np.array(Z)\n",
    "Z = np.column_stack([Z, np.sum(Z, axis=1) / 3])\n",
    "\n",
    "ax = axes[0]\n",
    "sns.heatmap(Z, annot=True, ax = ax, cmap=\"Reds\")\n",
    "ax.set_xticklabels(ALGO_NAMES + [\"Total\"])\n",
    "ax.set_yticklabels(ALGO_NAMES)\n",
    "ax.set_ylabel(\"Chosen Forest Type\")\n",
    "ax.set_xlabel(\"Training time spent per tree type\\n(relative to maximum possible)\")\n",
    "\n",
    "Z = []\n",
    "for i, a_choice in enumerate(ALGOS):\n",
    "    df_selection = df_comparison[(df_comparison[\"choice\"] == a_choice)]\n",
    "    Z.append([int(np.round(np.mean(df_selection[\"trees_\" + a_tree]))) for a_tree in ALGOS])\n",
    "Z = np.array(Z,dtype=int)\n",
    "Z = np.column_stack([Z, np.sum(Z, axis=1)])\n",
    "ax = axes[1]\n",
    "sns.heatmap(Z, annot=True, ax = ax, cmap=\"Reds\", fmt='g')\n",
    "ax.set_xticklabels(ALGO_NAMES + [\"Total\"])\n",
    "ax.set_yticklabels(ALGO_NAMES)\n",
    "ax.set_ylabel(\"Chosen forest type\")\n",
    "ax.set_xlabel(\"Numbers of trees grown per tree type\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compressions_in_time = [[np.mean(g[\"time_compression\"]) for i, g in df_comparison[df_comparison[\"choice\"] == a].groupby(\"openmlid\")] for a in ALGOS] + [[np.mean(g[\"time_compression\"]) for i, g in df_comparison.groupby(\"openmlid\")]]\n",
    "compressions_in_num_trees = [[np.mean(g[\"tree_compression\"]) for i, g in df_comparison[df_comparison[\"choice\"] == a].groupby(\"openmlid\")] for a in ALGOS] + [[np.mean(g[\"tree_compression\"]) for i, g in df_comparison.groupby(\"openmlid\")]]\n",
    "\n",
    "for a, values in zip(axes[2:], [compressions_in_time, compressions_in_num_trees]):\n",
    "    a.boxplot(values)\n",
    "    a.set_ylim([0, 1])\n",
    "    a.set_xticklabels(ALGO_NAMES + [\"Total\"], rotation=90)\n",
    "    a.axhline(0.5, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    a.axhline(0.25, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    a.axvline(4.5, color=\"black\", linewidth=1)\n",
    "axes[2].set_ylabel(\"Time Compression\")\n",
    "axes[3].set_ylabel(\"Tree Compression\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/computations.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ee1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_comparison.query(\"choice != 'rf'\")) / len(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a5f48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_figure_and_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        perf_selfopt = df_dataset[\"selfopt_test\"]\n",
    "        \n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[comp + \"_test\"]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances[comp].append(significant)\n",
    "        \n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = max(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        if False:\n",
    "\n",
    "\n",
    "            # format entries\n",
    "            formatted_vals = [f\"{np.round(100 * np.mean(v), 2)}$\\pm${np.round(100 * np.std(v), 1)}\" for i, v in enumerate(data_base)]\n",
    "            imps.append(scores_on_dataset[1] - scores_on_dataset[0])\n",
    "            for i, val in enumerate(formatted_vals):\n",
    "                if i in best_indices:\n",
    "                    formatted_vals[i] = \"\\\\textbf{\" + val + \"}\"\n",
    "                elif not significant:\n",
    "                    formatted_vals[i] = \"\\\\underline{\" + val + \"}\"\n",
    "\n",
    "            rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    imps = np.array(imps)\n",
    "    \n",
    "    for a in algo_names:\n",
    "        significances[a] = np.array(significances[a])\n",
    "    \n",
    "    \n",
    "    colors = {\n",
    "        \"rf\": \"C0\",\n",
    "        \"lda\": \"C1\",\n",
    "        \"pca\": \"C2\"\n",
    "    }\n",
    "    \n",
    "    # create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))#,  gridspec_kw={'width_ratios': [3, 1]})\n",
    "    \n",
    "    \n",
    "    for i, a in enumerate([\"rf\", \"lda\", \"pca\"]):\n",
    "        ax1.scatter(scores[significances[a],i], scores[significances[a],-1], s=20, color=colors[a])\n",
    "        ax1.scatter(scores[~significances[a],i], scores[~significances[a],-1], s=20, facecolors=\"None\", color=colors[a])\n",
    "    \n",
    "    ax1.plot([0.2,1], [0.2,1], linestyle=\"dotted\", color=\"black\")\n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel(\"Performance of Standard RF (blue), LDA RF (orange), and PCA RF (green)\")\n",
    "    ax1.set_ylabel(\"Performance of Self-Optimized RF\")\n",
    "    \n",
    "    imp_concated = []\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[:,i])\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[significances[\"rf\"],i])\n",
    "    \n",
    "    ax2.boxplot(imp_concated, vert=False)\n",
    "    ax2.axvline(-0.01, linestyle=\"dotted\", color=\"red\", linewidth=1)\n",
    "    ax2.axvline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.01, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.03, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    #ax2.set_yticklabels([\"Improvem.\", \"Significant Improvem.\"])\n",
    "    #ax2.scatter(list(range(len(imps))), imps)\n",
    "    #ax2.hist(imps, bins=200)\n",
    "    #ax2.set_yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return (fig, ax), pd.DataFrame(rows, columns=[\"openmlid\", \"rf_test\", \"selfopt_test\"]).to_latex(index = False, escape = False)\n",
    "    \n",
    "(fig, ax), df_latex = get_figure_and_table_for_paper(df_comparison)\n",
    "plt.show()\n",
    "print(df_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figure_and_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        perf_selfopt = df_dataset[\"selfopt_test\"]\n",
    "        \n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[comp + \"_test\"]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances[comp].append(significant)\n",
    "        \n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = max(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        if False:\n",
    "\n",
    "\n",
    "            # format entries\n",
    "            formatted_vals = [f\"{np.round(100 * np.mean(v), 2)}$\\pm${np.round(100 * np.std(v), 1)}\" for i, v in enumerate(data_base)]\n",
    "            imps.append(scores_on_dataset[1] - scores_on_dataset[0])\n",
    "            for i, val in enumerate(formatted_vals):\n",
    "                if i in best_indices:\n",
    "                    formatted_vals[i] = \"\\\\textbf{\" + val + \"}\"\n",
    "                elif not significant:\n",
    "                    formatted_vals[i] = \"\\\\underline{\" + val + \"}\"\n",
    "\n",
    "            rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    imps = np.array(imps)\n",
    "    \n",
    "    for a in algo_names:\n",
    "        significances[a] = np.array(significances[a])\n",
    "    \n",
    "    \n",
    "    colors = {\n",
    "        \"rf\": \"C2\",\n",
    "        \"lda\": \"C0\",\n",
    "        \"pca\": \"C1\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3),  gridspec_kw={'width_ratios': [1, 1]})\n",
    "    \n",
    "    \n",
    "    for i, a in enumerate([\"rf\", \"lda\", \"pca\"]):\n",
    "        ax1.scatter(scores[significances[a],i], scores[significances[a],-1], s=20, color=colors[a])\n",
    "        ax1.scatter(scores[~significances[a],i], scores[~significances[a],-1], s=20, facecolors=\"None\", color=colors[a])\n",
    "    \n",
    "    ax1.plot([0.2,1], [0.2,1], linestyle=\"dotted\", color=\"black\")\n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel(\"Accuracy of Standard RF (blue), LDA RF (orange), and PCA RF (green).\\nBullets show significant and circle statistically not significant differences.\")\n",
    "    ax1.set_ylabel(\"Accuracy of Self-Optimized RF\")\n",
    "    \n",
    "    imp_concated = []\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[:,i])\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[significances[algo],i])\n",
    "    \n",
    "    imp_concated.append(np.min(imps, axis=1))\n",
    "    \n",
    "    ax2.boxplot(imp_concated, vert=False)\n",
    "    ax2.axvline(-0.01, linestyle=\"dotted\", color=\"red\", linewidth=1)\n",
    "    ax2.axvline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.01, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.03, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.set_xlabel(\"Accuracy improvement achieved by SORF compared to ...\")\n",
    "    ax2.set_yticklabels(ALGO_NAMES + [name + \" (sign.)\" for name in ALGO_NAMES] + [\"Oracle\"])\n",
    "    #ax2.scatter(list(range(len(imps))), imps)\n",
    "    #ax2.hist(imps, bins=200)\n",
    "    #ax2.set_yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(np.mean(imp_concated[-1]))\n",
    "    \n",
    "    return (fig, ax), pd.DataFrame(rows, columns=[\"openmlid\", \"rf_test\", \"selfopt_test\"]).to_latex(index = False, escape = False)\n",
    "    \n",
    "(fig, ax), df_latex = get_figure_and_table_for_paper(df_comparison)\n",
    "fig.savefig(\"plots/performance.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "print(df_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figure_and_table_for_paper(df_comparison):\n",
    "    \n",
    "    \n",
    "    algo_names = [\"rf\", \"lda\", \"pca\"]\n",
    "    \n",
    "    scores = []\n",
    "    significances = {a : [] for a in algo_names}\n",
    "    \n",
    "    imps = []\n",
    "    \n",
    "    rows = []\n",
    "    for openmlid, df_dataset in df_comparison.groupby(\"openmlid\"):\n",
    "        perf_selfopt = df_dataset[\"selfopt_test\"]\n",
    "        \n",
    "        data_base = []\n",
    "        for comp in algo_names:\n",
    "            perf_comp = df_dataset[comp + \"_test\"]\n",
    "            data_base.append(perf_comp)\n",
    "            if np.linalg.norm(perf_comp - perf_selfopt) != 0:\n",
    "                significant = scipy.stats.wilcoxon(perf_comp, perf_selfopt).pvalue < 0.05\n",
    "            else:\n",
    "                significant = False\n",
    "            significances[comp].append(significant)\n",
    "        \n",
    "        data_base.append(perf_selfopt)\n",
    "\n",
    "        scores_on_dataset = [np.mean(v) for v in data_base]\n",
    "        imps_on_dataset = [scores_on_dataset[-1] - v for v in scores_on_dataset]\n",
    "        scores.append(scores_on_dataset)\n",
    "        imps.append(imps_on_dataset)\n",
    "        best_score = max(scores_on_dataset)\n",
    "        best_indices = [i for i in range(len(data_base)) if scores_on_dataset[i] == best_score]\n",
    "        if False:\n",
    "\n",
    "\n",
    "            # format entries\n",
    "            formatted_vals = [f\"{np.round(100 * np.mean(v), 2)}$\\pm${np.round(100 * np.std(v), 1)}\" for i, v in enumerate(data_base)]\n",
    "            imps.append(scores_on_dataset[1] - scores_on_dataset[0])\n",
    "            for i, val in enumerate(formatted_vals):\n",
    "                if i in best_indices:\n",
    "                    formatted_vals[i] = \"\\\\textbf{\" + val + \"}\"\n",
    "                elif not significant:\n",
    "                    formatted_vals[i] = \"\\\\underline{\" + val + \"}\"\n",
    "\n",
    "            rows.append([openmlid] + formatted_vals)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    imps = np.array(imps)\n",
    "    \n",
    "    for a in algo_names:\n",
    "        significances[a] = np.array(significances[a])\n",
    "    \n",
    "    \n",
    "    colors = {\n",
    "        \"rf\": \"C0\",\n",
    "        \"lda\": \"C1\",\n",
    "        \"pca\": \"C2\"\n",
    "    }\n",
    "    \n",
    "    # create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))#,  gridspec_kw={'width_ratios': [3, 1]})\n",
    "    \n",
    "    \n",
    "    for i, a in enumerate([\"rf\", \"lda\", \"pca\"]):\n",
    "        ax1.scatter(scores[significances[a],i], scores[significances[a],-1], s=20, color=colors[a])\n",
    "        ax1.scatter(scores[~significances[a],i], scores[~significances[a],-1], s=20, facecolors=\"None\", color=colors[a])\n",
    "    \n",
    "    ax1.plot([0.2,1], [0.2,1], linestyle=\"dotted\", color=\"black\")\n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel(\"Performance of Standard RF (blue), LDA RF (orange), and PCA RF (green)\")\n",
    "    ax1.set_ylabel(\"Performance of Self-Optimized RF\")\n",
    "    \n",
    "    imp_concated = []\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[:,i])\n",
    "    for i, algo in enumerate(algo_names):\n",
    "        imp_concated.append(imps[significances[\"rf\"],i])\n",
    "    \n",
    "    ax2.boxplot(imp_concated, vert=False)\n",
    "    ax2.axvline(-0.01, linestyle=\"dotted\", color=\"red\", linewidth=1)\n",
    "    ax2.axvline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.01, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax2.axvline(0.03, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    #ax2.set_yticklabels([\"Improvem.\", \"Significant Improvem.\"])\n",
    "    #ax2.scatter(list(range(len(imps))), imps)\n",
    "    #ax2.hist(imps, bins=200)\n",
    "    #ax2.set_yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return (fig, ax), pd.DataFrame(rows, columns=[\"openmlid\", \"rf_test\", \"selfopt_test\"]).to_latex(index = False, escape = False)\n",
    "    \n",
    "(fig, ax), df_latex = get_figure_and_table_for_paper(df_comparison)\n",
    "plt.show()\n",
    "print(df_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6bc625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "100* np.round(df_comparison[[\"openmlid\", \"rf_test\", \"selfopt_test\"]].groupby(\"openmlid\").mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gaps))\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].boxplot(gaps)\n",
    "ax[1].boxplot(improvements_over_rf)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].hist(choices)\n",
    "ax[1].hist(best_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a62b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for openmlid in sorted(pd.unique(bm.cache[\"openmlid\"])):\n",
    "    fig, ax = plt.subplots()\n",
    "    sizes = list(range(1,201))\n",
    "    max_val = 0\n",
    "    for i, algo in enumerate([\"rf\", \"lda\", \"pca\"]):\n",
    "        for test, linestyle in zip([True, False], [\"solid\", \"--\"]):\n",
    "            curves = []\n",
    "            for seed in range(1):\n",
    "                curve = bm.get_curve(openmlid, algo, seed, oob = not test)\n",
    "                curves.append(list(curve[1]))\n",
    "            if not curves:\n",
    "                continue\n",
    "            curves = np.array(curves)\n",
    "            means = np.mean(curves, axis=0)\n",
    "            q3 = np.percentile(curves, 25, axis=0)\n",
    "            q7 = np.percentile(curves, 75, axis=0)\n",
    "            max_val = max(max_val, max(means))\n",
    "            ax.plot(sizes, means, color=f\"C{i}\", linestyle=linestyle)\n",
    "            ax.fill_between(sizes, q3, q7, color=f\"C{i}\", alpha=0.2)\n",
    "    #ax.set_ylim([max_val - 0.1, max_val + 0.01])\n",
    "    ax.set_title(openmlid)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
